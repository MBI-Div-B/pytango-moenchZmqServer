{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MOENCH ZMQ TangoDS documentation!","text":"<p>This documentation describes the custom implementation of the ZMQ server for the MOENCH detector made in Paul Scherrer Institut in Villigen, Switzerland.</p> <p>The original version of the software for MOENCH detectors is published in the SLS detector group repository and described in the corresponding documentation. </p> <p>This version is done especially for MOENCH03 prototype and can be possibly adapted for the next revisions of MOENCH family detectors (see Detector reference).</p> <p>The core features of this project are:</p> <ol> <li>tight integration of the processing software with the tango-controls framework</li> <li>pure python codebase</li> <li>unified interface for any kind of processing algorithm</li> <li>easier modification and adjustment of the software</li> </ol> <p>The current project, written in Python, is maintained by Leonid Lunin at the Max-Born-Institute, Berlin, Germany.</p>"},{"location":"detector/","title":"Detector reference","text":"<p>Detector sends two separate ZMQ messages for each acquired frame coming after each other. </p>"},{"location":"detector/#json-header","title":"JSON header","text":"<p>The first one is a JSON header where all necessary information is stored.</p> <p>This is the JSON header signature for the firmware version 6.1.2 for a MOENCH detector: <pre><code>{\n\"jsonversion\": unsigned int,\n\"bitmode\": unsigned int,\n\"fileIndex\": unsigned long int,\n\"detshape\": [\nunsigned int,\nunsigned int\n],\n\"shape\": [\nunsigned int,\nunsigned int\n],\n\"size\": unsigned int,\n\"acqIndex\": unsigned long int,\n\"frameIndex\": unsigned long int,\n\"progress\": double,\n\"fname\": string,\n\"data\": unsigned int,\n\"completeImage\": unsigned int,\n\n\"frameNumber\": unsigned long long int,\n\"expLength\": unsigned int,\n\"packetNumber\": unsigned int,\n\"detSpec1\": unsigned long int,\n\"timestamp\": unsigned long int,\n\"modId\": unsigned int,\n\"row\": unsigned int,\n\"column\": unsigned int,\n\"detSpec2\": unsigned int,\n\"detSpec3\": unsigned int,\n\"detSpec4\": unsigned int,\n\"detType\": unsigned int,\n\"version\": unsigned int,\n\n\"flipRows\": unsigned int,\n\"quad\": unsigned int,\n\"addJsonHeader\": {\nstring : string\n}\n}\n</code></pre> with a comprehensive description for each field </p> Field Description jsonversion Version of the json header. Value at 4 for v6.x.x   and v7.x.x bitmode Bits per pixel [4|8|16|32] fileIndex Current file acquisition index detshape Geometry of the entire detector shape Geometry of the current port streamed out size Size of image of current port in bytesout acqIndex Frame number from the detector (redundant) frameIndex Frame number of current acquisition (Starting at 0) progress Progress of current acquisition in % fname Current file name data 1 if there is data following 0 if dummy header completeImage 1 if no missing packets for this frame in this port,   else 0 frameNumber Frame number [From detector udp header] expLength subframe number (32 bit eiger) or real time exposure   time in 100ns (others) [From detector udp header] packetNumber Number of packets caught for that frame detSpec1 See here [From detector udp header] timestamp Timestamp with 10 MHz clock [From detector udp   header] modId Module Id [From detector udp header] row Row number in detector [From detector udp header] column Column number in detector [From detector udp header] detSpec2 See here [From detector udp header] detSpec3 See here [From detector udp header] detSpec4 See here [From detector udp header] detType detSpec3 Detector type enum See Detector enum [From   detector udp header] version Detector header version. At 2 [From detector udp   header] flipRows 1 if rows should be flipped. Usually for Eiger bottom. quad 1 if its an Eiger quad. addJsonHeader Optional custom parameters that is required for   processing code. <p>Here is an example of the received message from MOENCH detector: <pre><code>{\n\"jsonversion\": 4,\n\"bitmode\": 16,\n\"fileIndex\": 6,\n\"detshape\": [1, 1],\n\"shape\": [400, 400],\n\"size\": 320000,\n\"acqIndex\": 1,\n\"frameIndex\": 0,\n\"progress\": 100.0,\n\"fname\": \"/mnt/LocalData/DATA/MOENCH/20230128_run/230128\",\n\"data\": 1,\n\"completeImage\": 1,\n\"frameNumber\": 1,\n\"expLength\": 0,\n\"packetNumber\": 40,\n\"bunchId\": 0,\n\"timestamp\": 0,\n\"modId\": 0,\n\"row\": 0,\n\"column\": 0,\n\"reserved\": 0,\n\"debug\": 0,\n\"roundRNumber\": 0,\n\"detType\": 5,\n\"version\": 1,\n\"flipRows\": 0,\n\"quad\": 0,\n\"addJsonHeader\": {\"detectorMode\": \"analog\", \"frameMode\": \"raw\"},\n}\n</code></pre> At the end of each acquisition the MOENCH detector sends a dummy header where almost all fields (the most important one is <code>\"data\" : 0</code>) are zeros or empty. After a dummy JSON header no frame of the detector will be transmitted and should not be awaited by the server.</p>"},{"location":"detector/#changes-between-the-firmware-versions","title":"Changes between the firmware versions","text":"<p>Changes from  6.x.x to 7.0.0: 4 field names have changed from 6.x.x to 7.x.x because they have also been changed in the detector udp header.  Since the meaning has not changed, the udp header version stays the same as well as the  zmq header version. detSpec1 &lt;- bunchId detSpec2 &lt;- reserved detSpec3 &lt;- debug detSpec4 &lt;- roundRNumber</p>"},{"location":"detector/#payload","title":"Payload","text":"<p>The second ZMQ packet consists of the acquired capture represented as a 1D array of bytes where 160000 values are stored in a <code>unsigned 16-bit integer</code> (<code>np.uint16</code>) data type. Nevertheless, it is not a just flatten 400x400 2D array where the pixels line up one by one but in a strange order defined (as I understand) by reading from the ADC on the detector's FPGA board.</p>"},{"location":"detector/#frame-remapping","title":"Frame remapping","text":"<p>In the main repository <code>slsDetectorPackage</code> we can find a reference <code>moench03T1ReceiverDataNew.h#66:110</code> which reoders the pixels in a frame: <pre><code>int nadc = 32;\nint sc_width = 25;\nint sc_height = 200;\n\nint adc_nr[32] = {300, 325, 350, 375, 300, 325, 350, 375, 200, 225, 250,\n275, 200, 225, 250, 275, 100, 125, 150, 175, 100, 125,\n150, 175, 0,   25,  50,  75,  0,   25,  50,  75};\n\nint row, col;\n\nint isample;\nint iadc;\nint ix, iy;\n\nint npackets = 40;\nint i;\nint adc4(0);\nint off=sizeof(header);\nfor (int ip = 0; ip &lt; npackets; ip++) {\nfor (int is = 0; is &lt; 128; is++) {\n\nfor (iadc = 0; iadc &lt; nadc; iadc++) {\ni = 128 * ip + is;\nadc4 = (int)iadc / 4;\nif (i &lt; sc_width * sc_height) {\n//  for (int i=0; i&lt;sc_width*sc_height; i++) {\ncol = adc_nr[iadc] + (i % sc_width);\nif (adc4 % 2 == 0) {\nrow = 199 - i / sc_width;\n} else {\nrow = 200 + i / sc_width;\n}\ndataMap[row][col] = off +\n(nadc * i + iadc) * 2; //+16*(ip+1);\n#ifdef HIGHZ\ndataMask[row][col] = 0x3fff; // invert data\n#endif\nif (dataMap[row][col] &lt; 0 ||\ndataMap[row][col] &gt;= off + nSamples * 2 * 32)\nstd::cout &lt;&lt; \"Error: pointer \" &lt;&lt; dataMap[row][col]\n&lt;&lt; \" out of range \" &lt;&lt; std::endl;\n}\n}\n}\n}\n</code></pre> As the variables used above depend on the type of detector, they remain constant for a single detector model. This allows us to compute this \"pixel rearrangement map\" just once and then use it for each frame.</p> <p>So I just rewrite the upper C++ code in python and saved the <code>reorder_map</code> which is basically a 400x400 <code>numpy</code> array which consists of indexes. <pre><code>import numpy as np\n\n# 32 numbers\nnadc = 32\nadc_nr = [300, 325, 350, 375, 300, 325, 350, 375,\n    200, 225, 250, 275, 200, 225, 250, 275,\n    100, 125, 150, 175, 100, 125, 150, 175,\n    0, 25, 50, 75, 0, 25, 50, 75,\n]\nnpackets = 40\nsc_width = 25\nsc_height = 200\n\n# indexes need to be integer\nind = np.zeros([400, 400], dtype=np.int32)\nfor ip in range(npackets):\n    for iss in range(128):\n        for iadc in range(32):\n            i = 128 * ip + iss\n            adc4 = iadc // 4\n            if i &lt; sc_width * sc_height:\n                col = adc_nr[iadc] + (i % sc_width)\n                if adc4 % 2 == 0:\n                    row = 199 - i // sc_width\n                else:\n                    row = 200 + i // sc_width\n                ind[row, col] = 32 * i + iadc\nnp.save(\"reorder_map\", ind)\n</code></pre> Thanks to the built-in <code>numpy</code> feature, we can easily rearrange the 1D array coming from the detector into 2D with the right order: <pre><code># load the map once\nreorder_map = np.load(\"reorder_map.npy\")\n\n# obtain the 1D array represents frame from ZMQ \nmsg = socket.recv()\n# saving a raw 1D array\nraw_frame = np.frombuffer(msg, dtype=np.uint16)\n# rearrange the frame in the right order\nframe = raw_frame[reorder_map]\n</code></pre></p>"},{"location":"guideline/","title":"Coding guideline","text":""},{"location":"guideline/#asynchronous-io","title":"Asynchronous IO","text":"<p>First introduced in Python 3.8, asynchronous IO allow you to create server clients in a much more elegant way.</p> <p>The <code>asyncio</code> philosophy differs from the convenient programming style and can look confusing at the very beginning. I would recommend this learn materials to make yourself familiar with the basics:</p> <ul> <li>26 min video with basics</li> <li>Official python 3.10 asyncio documentation </li> </ul> <p>In case of this particular server it is possible to handle ZMQ stream in more appropriate way. Instead of continouns polling of messages from ZMQ stream</p> <p>before: <pre><code>context = zmq.Context()\nsocket = context.socket(zmq.SUB)\nwhile True:\n    try:\n        msg = socket.recv(flags=zmq.NOBLOCK)\n    except zmq.ZMQError:\n        # no packet more in the stream\n    time.sleep(0.25)\n</code></pre></p> <p>they can be received asynchronously that program \"awaits\" each oncoming message</p> <p>after: <pre><code>context = zmq.asyncio.Context()\nsocket = context.socket(zmq.SUB)\nwhile True:\n    msg = await socket.recv()\n</code></pre></p> <p>The TangoDS can work in asynchronous IO mode too if the variable <code>green_mode</code> set to <code>tango.GreenMode.Asyncio</code>. In this case we are able to use the save <code>event_loop</code> for asynchronous receive of ZMQ packets from detector and tango communication as well.</p>"},{"location":"guideline/#multiprocessing","title":"Multiprocessing","text":"<p>Usually concurrency in most of programming languages (as Java, or C++) is using threads. One of the features of the python language, namely the implementation of its default interpreter CPython is the so-called global interpreter lock (GIL) which actually allows to run only one thread at the same time. However, there are ways to bypass this feature with standard language methods.</p> <p>There is a <code>multiprocessing</code> package for this, which creates additional processes (which are not actually threads but do the same thing). There are various high API tools to make a program which utilizes more than one core of the CPU.</p>"},{"location":"guideline/#thread-safe-variables","title":"Thread-safe variables","text":"<p>The main difference in the development of the multiprocessing programs compared to the single thread programs is the synchronization between processes. This means that even the most commong operations as reading or assigning a variable's value need to be thread-safe.</p> <p>Here is an example how a simple incrementing the value <code>counter</code> from by 1 two processes can cause a problem: sequenceDiagram     Process A-&gt;&gt;+Memory: read counter value     Memory-&gt;&gt;Process A: read counter = 0     Process B-&gt;&gt;+Memory: read counter value     Memory-&gt;&gt;Process B: read counter = 0     Process A--&gt;Process A: incrementing counter by 1     Process A-&gt;&gt;+Memory: write counter =  1     Process B--&gt;Process B: incrementing counter by 1     Process B-&gt;&gt;+Memory: write counter =  1  So the <code>counter</code> value which expected to be 2 will be 1. The reason of it is the fact that incrementing a variable is not an atomic operation itself and consists of one read operation and then write operation. Actually (depending on the CPU and programming language architecture) read and write operations itself can be not atomic. Thus, we need a synchronization between two threads in order to perform the operations in the right order.</p> <p>The most trivial example is so called mutex. This is an object shared between processes which can be acquired or released. While the shared lock is acquired by one of the processes the other processes will wait till it is released. Here is the upper example with using a lock:</p> <p>sequenceDiagram     Process A-&gt;&gt;+Shared lock: acquire lock     Process B--&gt;&gt;Shared lock: try acquire lock     Shared lock--&gt;&gt;Process B: lock is already acquired by Process A     Process A-&gt;&gt;+Memory: read counter value     Memory-&gt;&gt;Process A: read counter = 0     Process A--&gt;Process A: incrementing counter by 1     Process A-&gt;&gt;+Memory: write counter =  1     Process A-&gt;&gt;+Shared lock: release lock     Process B-&gt;&gt;+Shared lock: acquire lock     Process B-&gt;&gt;+Memory: read counter value     Memory-&gt;&gt;Process B: read counter = 1     Process B--&gt;Process B: incrementing counter by 1     Process B-&gt;&gt;+Memory: write counter =  2     Process B-&gt;&gt;+Shared lock: release lock  The classes for synchronization are available in <code>multiprocessing</code> library. Here is the upper example implemented in python:</p> <pre><code># initialize a shared lock instance\nshared_lock = multiprocessing.Lock()\n\n# initialize a shared integer variable\nshared_counter = multiprocessing.Value(\"i\", 0)\n\n# function which will be executed in the process A\ndef func_A(lock, counter):\n    lock.acquire()\n    counter += 1\n    print(counter)\n    lock.release()\n\n# function which will be executed in the process B\ndef func_B(lock, counter):\n    lock.acquire()\n    counter += 1\n    print(counter)\n    lock.release()\n\n# create a pool with two processes\nprocess_pool = multiprocessing.ProcessPoolExecutor(2)\n\n# assign to run a function func_A with\n# the args shared_lock and shared_counter with one of the process in pool\nprocess_pool.submit(func_A, shared_lock, shared_counter)\n\n# assign to run a function func_B with\n# the args shared_lock and shared_counter with one of the process in pool\nprocess_pool.submit(func_B, shared_lock, shared_counter)\n\n# Output:\n# 1\n# 2\n</code></pre> <p>However, there are only basic data types as scalar <code>Value</code> and only 1D array <code>Array</code> available. The  $n \\times m$ 2D array can be possibly stored as 1D array with the length of $l = n \\cdot m$. But it is painful to read, reshape, perform an operation, reshape again and store an array. Is it possible to have a shared <code>numpy</code> array?</p> <p>Yes, but it is a little bit complex In python 3.8 a shared memory feature was introduced and this allows to work directly with a memory space and use than assign a <code>numpy</code> array to work with it. Here is an example of creating this kind of <code>numpy</code>array: <pre><code># a manager which allocates the shared memory\nshared_memory_manager =  multiprocessing.managers.SharedMemoryManager()\n# get how many bytes need to be allocated and shared for an 400x400 float numpy array\n# the values of this array will be never used \narray_length = np.zeros([400, 400], dtype=float).nbytes\n# saving the pointer to the shared memory\nmem_pointer = shared_memory_manager.SharedMemory(size=array_length)\n# assign to a numpy array to store its values\n# under the given shared memory address\nnumpy_array = np.ndarray((400, 400), dtype=float, buffer=mem_pointer.buf)\n</code></pre></p> <p>Learn more:</p> <ul> <li>Official python 3.10 shared memory documentation</li> <li>Comprehensive example</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#dependencies","title":"Dependencies","text":"<ul> <li>python &gt;=3.10</li> <li>pyzmq</li> <li>tornado</li> <li>pytango</li> <li>Pillow</li> <li>scipy</li> <li>skicit-image</li> </ul> <p>Warning</p> <p>Please note that this TangoDS explicitly uses <code>asyncio</code> API firstly introduced in python 3.8. However, there are several bugs in the python 3.8 which do not allow to use this TangoDS properly. These bugs were fixed in the version 3.10 of python. Since this version is required some python 3.10 specific features were used in the code.</p>"},{"location":"installation/#initial-setup","title":"Initial setup","text":"<p>To use this TangoDS you need to firstly register it in your Tango database. One of options is to use Jive.</p> <p>The following TangoDS properties should be specified: </p> Property name Datatype Description Default value <code>ZMQ_RX_IP</code> <code>String</code> IP address where the detector opens ZMQ socket, must match the detector's config <code>\"192.168.2.200\"</code> (workstation's IP on 10Gb interface) <code>ZMQ_RX_PORT</code> <code>String</code> Port where the detector opens ZMQ socket, must match the detector's config <code>\"50003\"</code> <code>PROCESSING_CORES</code> <code>Integer</code> Amount of cores utilized/processes spawned for parallel processing of oncoming images <code>20</code> <code>FLIP_IMAGE</code> <code>Boolean</code> Flip the stored images in the <code>read_</code> functions of the TangoDS. Due to the other specification used for detector the oncoming image from the detector is flipped vertically. Please note that the images are stored in unflipped state and will be flipped only while reading. <code>true</code>"},{"location":"reference/","title":"Code reference","text":"<p>         Bases: <code>Device</code></p> <p>Custom implementation of zmq processing server for X-ray detector MOENCH made in PSI which is integrated with a Tango device server.</p> Source code in <code>pytango-moenchZmqServer\\MoenchZmqServer.py</code> <pre><code>class MoenchZmqServer(Device):\n\"\"\"Custom implementation of zmq processing server for X-ray detector MOENCH made in PSI which is integrated with a Tango device server.\"\"\"\n\n    processing_function = None\n    processing_function_enum = ProcessingMode(0)\n\n    _manager = None\n    _context = None\n    _socket = None\n    _process_pool = None\n    green_mode = GreenMode.Asyncio\n\n    # probably should be rearranged in array, because there will pumped and unpumped images, for each type of processing\n    # and further loaded with dynamic attributes\n    shared_memory_pedestal = None\n    shared_memory_analog_img = None\n    shared_memory_threshold_img = None\n    shared_memory_counting_img = None\n\n    shared_threshold = None\n    shared_counting_threshold = None\n    shared_processed_frames = None\n    shared_amount_frames = None\n    shared_server_running = False\n    shared_split_pump = None\n\n    reorder_table = None\n\n    _save_analog_img = True\n    _save_threshold_img = True\n    _save_counting_img = True\n\n    ZMQ_RX_IP = device_property(\n        dtype=str,\n        doc=\"port of the slsReceiver instance, must match the config\",\n        default_value=\"192.168.2.200\",\n    )\n    ZMQ_RX_PORT = device_property(\n        dtype=str,\n        doc=\"ip of slsReceiver instance, must match the config\",\n        default_value=\"50003\",\n    )\n    PROCESSING_CORES = device_property(\n        dtype=int,\n        doc=\"cores amount to process, up to 72 on MOENCH workstation\",\n        default_value=20,\n    )\n    FLIP_IMAGE = device_property(\n        dtype=bool,\n        doc=\"should the final image be flipped/inverted along y-axis\",\n        default_value=True,\n    )\n\n    pedestal = attribute(\n        display_level=DispLevel.EXPERT,\n        label=\"pedestal\",\n        dtype=float,\n        dformat=AttrDataFormat.IMAGE,\n        max_dim_x=400,\n        max_dim_y=400,\n        access=AttrWriteType.READ_WRITE,\n        doc=\"pedestal (averaged dark images), i.e. offset which will be subtracted from each acquired picture\",\n    )\n    analog_img = attribute(\n        display_level=DispLevel.EXPERT,\n        label=\"analog img\",\n        dtype=float,\n        dformat=AttrDataFormat.IMAGE,\n        max_dim_x=400,\n        max_dim_y=400,\n        access=AttrWriteType.READ,\n        doc=\"sum of images processed with subtracted pedestals\",\n    )\n    threshold_img = attribute(\n        display_level=DispLevel.EXPERT,\n        label=\"threshold img\",\n        dtype=float,\n        dformat=AttrDataFormat.IMAGE,\n        max_dim_x=400,\n        max_dim_y=400,\n        access=AttrWriteType.READ,\n        doc='sum of \"analog images\" (with subtracted pedestal) processed with thresholding algorithm',\n    )\n    counting_img = attribute(\n        display_level=DispLevel.EXPERT,\n        label=\"counting img\",\n        dtype=float,\n        dformat=AttrDataFormat.IMAGE,\n        max_dim_x=400,\n        max_dim_y=400,\n        access=AttrWriteType.READ,\n        doc='sum of \"analog images\" (with subtracted pedestal) processed with counting algorithm',\n    )\n\n    threshold = attribute(\n        label=\"th\",\n        unit=\"ADU\",\n        dtype=float,\n        min_value=0.0,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        doc=\"cut-off value for thresholding\",\n    )\n    counting_threshold = attribute(\n        label=\"counting th\",\n        unit=\"ADU\",\n        dtype=float,\n        min_value=0.0,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        doc=\"cut-off value for counting\",\n    )\n    processing_mode = attribute(\n        label=\"mode\",\n        dtype=ProcessingMode,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        fisallowed=\"isWriteAvailable\",\n        doc=\"mode of frames processing [ANALOG = 0, THRESHOLD = 1, COUNTING = 2, PEDESTAL = 3]\",\n    )\n\n    processed_frames = attribute(\n        label=\"proc frames\",\n        dtype=int,\n        access=AttrWriteType.READ,\n        doc=\"amount of already processed frames\",\n    )\n    amount_frames = attribute(\n        label=\"expected frames\",\n        dtype=int,\n        access=AttrWriteType.READ_WRITE,\n        doc=\"expected frames to receive from detector\",\n    )\n\n    server_running = attribute(\n        display_level=DispLevel.EXPERT,\n        label=\"is server running?\",\n        dtype=bool,\n        access=AttrWriteType.READ,\n        doc=\"if true - server is running, otherwise - not\",\n    )\n\n    # split_pump = attribute(\n    #     label=\"split (un)pumped\",\n    #     dtype=bool,\n    #     access=AttrWriteType.READ_WRITE,\n    #     memorized=True,\n    #     hw_memorized=True,\n    #     doc=\"split odd and even frames\",\n    # )\n\n    save_analog_img = attribute(\n        label=\"save analog\",\n        dtype=bool,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        doc=\"save analog .tiff file after acquisition\",\n    )\n\n    save_threshold_img = attribute(\n        label=\"save threshold\",\n        dtype=bool,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        doc=\"save threshold .tiff file after acquisition\",\n    )\n\n    save_counting_img = attribute(\n        label=\"save counting\",\n        dtype=bool,\n        access=AttrWriteType.READ_WRITE,\n        memorized=True,\n        hw_memorized=True,\n        doc=\"save counting .tiff file after acquisition\",\n    )\n\n    def write_pedestal(self, value):\n        self.shared_pedestal.value = value\n\n    def read_pedestal(self):\n        return self._read_shared_array(\n            shared_memory=self.shared_memory_pedestal, flip=self.FLIP_IMAGE\n        )\n\n    def write_analog_img(self, value):\n        self._write_shared_array(\n            shared_memory=self.shared_memory_analog_img, value=value\n        )\n\n    def read_analog_img(self):\n        return self._read_shared_array(\n            shared_memory=self.shared_memory_analog_img, flip=self.FLIP_IMAGE\n        )\n\n    def write_threshold_img(self, value):\n        self._write_shared_array(\n            shared_memory=self.shared_memory_threshold_img, value=value\n        )\n\n    def read_threshold_img(self):\n        return self._read_shared_array(\n            shared_memory=self.shared_memory_threshold_img, flip=self.FLIP_IMAGE\n        )\n\n    def write_counting_img(self, value):\n        self._write_shared_array(\n            shared_memory=self.shared_memory_counting_img, value=value\n        )\n\n    def read_counting_img(self):\n        return self._read_shared_array(\n            shared_memory=self.shared_memory_counting_img, flip=self.FLIP_IMAGE\n        )\n\n    def write_threshold(self, value):\n        self.shared_threshold.value = value\n\n    def read_threshold(self):\n        return self.shared_threshold.value\n\n    def write_counting_threshold(self, value):\n        self.shared_counting_threshold.value = value\n\n    def read_counting_threshold(self):\n        return self.shared_counting_threshold.value\n\n    def write_processing_mode(self, value):\n        # matching values and functions [ANALOG = 0, THRESHOLD = 1, COUNTING = 2]\n        self.processing_function_enum = ProcessingMode(value)\n        match self.processing_function_enum:\n            case ProcessingMode.ANALOG:\n                self.processing_function = processing_functions.analog\n            case ProcessingMode.THRESHOLD:\n                self.processing_function = processing_functions.thresholding\n            case ProcessingMode.COUNTING:\n                self.processing_function = processing_functions.counting\n\n    def read_processing_mode(self):\n        return self.processing_function_enum\n\n    def write_processed_frames(self, value):\n        self.shared_processed_frames.value = value\n\n    def read_processed_frames(self):\n        return self.shared_processed_frames.value\n\n    def write_amount_frames(self, value):\n        self.shared_amount_frames.value = value\n\n    def read_amount_frames(self):\n        return self.shared_amount_frames.value\n\n    def write_server_running(self, value):\n        self.shared_server_running.value = int(value)\n\n    def read_server_running(self):\n        return bool(self.shared_server_running.value)\n\n    def write_save_analog_img(self, value):\n        self._save_analog_img = value\n\n    def read_save_analog_img(self):\n        return self._save_analog_img\n\n    def write_save_threshold_img(self, value):\n        self._save_threshold_img = value\n\n    def read_save_threshold_img(self):\n        return self._save_threshold_img\n\n    def write_save_counting_img(self, value):\n        self._save_counting_img = value\n\n    def read_save_counting_img(self):\n        return self._save_counting_img\n\n    # when processing is ready -&gt; self.push_change_event(self, \"analog_img\"/\"counting_img\"/\"threshold_img\")\n\n    async def main(self):\n        while True:\n            header, payload = await self.get_msg_pair()\n            if payload is not None:\n                # def wrap_function(\n                # mode,\n                # header,\n                # payload,\n                # lock,\n                # shared_memories,\n                # processed_frames,\n                # amount_frames,\n                # frame_func,\n                # threshold,\n                # counting_threshold,\n                # *args,\n                # **kwargs)\n                self.shared_received_frames.value += 1\n                future = self._process_pool.submit(\n                    wrap_function,\n                    self.processing_function_enum,\n                    header,\n                    payload.astype(float),\n                    self._lock,\n                    [\n                        self.shared_memory_analog_img,\n                        self.shared_memory_threshold_img,\n                        self.shared_memory_counting_img,\n                        self.shared_memory_pedestal,\n                    ],  # need to changed corresponding to the frame_func\n                    self.shared_processed_frames,\n                    self.shared_received_frames,\n                    self.processing_function,\n                    self.shared_threshold,\n                    self.shared_counting_threshold,\n                )\n                future = asyncio.wrap_future(future)\n\n    async def get_msg_pair(self):\n        isNextPacketData = True\n        header = None\n        payload = None\n        packet1 = await self._socket.recv()\n        try:\n            print(\"parsing header...\")\n            header = json.loads(packet1)\n            print(header)\n            isNextPacketData = header.get(\"data\") == 1\n            print(f\"isNextPacketdata {isNextPacketData}\")\n        except:\n            print(\"is not header\")\n            isNextPacketData = False\n        if isNextPacketData:\n            print(\"parsing data...\")\n            packet2 = await self._socket.recv()\n            payload = np.zeros([400, 400], dtype=np.uint16)\n            raw_buffer = np.frombuffer(packet2, dtype=np.uint16)\n            # see in docs\n            # should be moved to each process to prevent performance bottleneck\n            payload = raw_buffer[self.reorder_table]\n        return header, payload\n\n    def _read_shared_array(self, shared_memory, flip: bool):\n        self._lock.acquire()\n        buf = np.ndarray((400, 400), dtype=float, buffer=shared_memory.buf)\n        array = np.copy(buf)\n        self._lock.release()\n        if flip:\n            return np.flipud(array)\n        else:\n            return array\n\n    def _write_shared_array(self, shared_memory, value):\n        self._lock.acquire()\n        array = np.ndarray((400, 400), dtype=float, buffer=shared_memory.buf)\n        array = value\n        self._lock.release()\n\n    @command\n    def start_receiver(self):\n        empty = np.zeros([400, 400], dtype=float)\n        self.write_server_running(True)\n        self.write_processed_frames(0)\n        self.write_analog_img(empty)\n        self.write_counting_img(empty)\n        self.write_threshold_img(empty)\n        self.set_state(DevState.RUNNING)\n\n    @command\n    async def stop_receiver(self):\n        received_frames = self.shared_received_frames.value\n        print(f\"received {received_frames} frames\")\n        loop = asyncio.get_event_loop()\n        loop.create_task(self.async_stop_receiver(received_frames))\n\n    async def async_stop_receiver(self, received_frames):\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self.block_stop_receiver, received_frames)\n\n    def block_stop_receiver(self, received_frames_at_the_time):\n        while received_frames_at_the_time != self.shared_processed_frames.value:\n            print(\"not all frames processed yet\")\n            time.sleep(2)\n        self.write_server_running(False)\n        self.set_state(DevState.ON)\n        print(\"all frames processed\")\n        # HERE ALL POST HOOKS\n        # if processmode == pedestal:\n        # shared_pedestal / received frames\n\n    @command\n    def acquire_pedestals(self):\n        pass\n\n    def init_device(self):\n\"\"\"Initial tangoDS setup\"\"\"\n        Device.init_device(self)\n        self.set_state(DevState.INIT)\n        self.get_device_properties(self.get_device_class())\n\n        self.reorder_table = np.load(\"reorder_table.npy\")\n        # sync manager for synchronization between threads\n        self._manager = mp.Manager()\n        # using simple mutex (lock) to synchronize\n        self._lock = self._manager.Lock()\n\n        # manager for allocation of shared memory between threads\n        self._shared_memory_manager = SharedMemoryManager()\n        # starting the shared memory manager\n        self._shared_memory_manager.start()\n        # default values of properties do not work without database though \u00af\\_(\u30c4)_/\u00af\n        processing_cores_amount = 16  # self.PROCESSING_CORES\n        zmq_ip = \"127.0.0.1\"  # self.ZMQ_RX_IP\n        zmq_port = \"50003\"  # self.ZMQ_RX_PORT\n\n        # using shared threadsafe Value instance from multiprocessing\n        self.shared_threshold = self._manager.Value(\"f\", 0)\n        self.shared_counting_threshold = self._manager.Value(\"f\", 0)\n        self.shared_server_running = self._manager.Value(\"b\", 0)\n        self.shared_processed_frames = self._manager.Value(\"I\", 0)\n        self.shared_received_frames = self._manager.Value(\"I\", 0)\n        self.shared_amount_frames = self._manager.Value(\"I\", 0)\n        self.shared_split_pump = self._manager.Value(\"b\", 0)\n\n        # calculating how many bytes need to be allocated and shared for a 400x400 float numpy array\n        img_bytes = np.zeros([400, 400], dtype=float).nbytes\n        # allocating 4 arrays of this type\n        self.shared_memory_pedestal = self._shared_memory_manager.SharedMemory(\n            size=img_bytes\n        )\n        self.shared_memory_analog_img = self._shared_memory_manager.SharedMemory(\n            size=img_bytes\n        )\n        self.shared_memory_threshold_img = self._shared_memory_manager.SharedMemory(\n            size=img_bytes\n        )\n        self.shared_memory_counting_img = self._shared_memory_manager.SharedMemory(\n            size=img_bytes\n        )\n        # creating thread pool executor to which the frame processing will be assigned\n        self._process_pool = ProcessPoolExecutor(processing_cores_amount)\n\n        # creating and initialing socket to read from\n        self._init_zmq_socket(zmq_ip, zmq_port)\n        loop = asyncio.get_event_loop()\n        loop.create_task(self.main())\n\n        # initialization of tango events for pictures buffers\n        self.set_change_event(\"analog_img\", True, False)\n        self.set_change_event(\"threshold_img\", True, False)\n        self.set_change_event(\"counting_img\", True, False)\n        self.set_state(DevState.ON)\n\n    # updating of tango events for pictures buffers\n    @command\n    def update_images_events(self):\n        self.push_change_event(\"analog_img\", self.read_analog_img(), 400, 400),\n        self.push_change_event(\"threshold_img\", self.read_threshold_img(), 400, 400)\n        self.push_change_event(\"counting_img\", self.read_counting_img(), 400, 400)\n\n    # save files on disk for pictures buffers\n    def save_files(self, path, filename, index):\n\"\"\"Function for saving the buffered images in .tiff format.\n        The files will have different postfixes depending on processing mode.\n\n        Args:\n            path (str): folder to save\n            filename (str): name to save\n            index (str): capture index\n        \"\"\"\n        savepath = os.path.join(path, filename)\n        if self.read_save_analog_img():\n            im = Image.fromarray(self.read_analog_img())\n            im.save(f\"{savepath}_{index}_analog.tiff\")\n\n        if self.read_save_threshold_img():\n            im = Image.fromarray(self.read_threshold_img())\n            im.save(f\"{savepath}_{index}_threshold_{self.read_threshold()}.tiff\")\n\n        if self.read_save_counting_img():\n            im = Image.fromarray(self.read_analog_img())\n            im.save(\n                f\"{savepath}_{index}_counting_{self.read_counting_threshold()}.tiff\"\n            )\n\n    def _init_zmq_socket(self, zmq_ip: str, zmq_port: str):\n        endpoint = f\"tcp://{zmq_ip}:{zmq_port}\"\n        self._context = zmq.asyncio.Context()\n        self._socket = self._context.socket(zmq.SUB)\n        print(f\"Connecting to: {endpoint}\")\n        self._socket.connect(endpoint)\n        self._socket.setsockopt(zmq.SUBSCRIBE, b\"\")\n\n    def delete_device(self):\n        self._process_pool.shutdown()\n        self._manager.shutdown()\n        self._shared_memory_manager.shutdown()\n</code></pre>"},{"location":"reference/#MoenchZmqServer.MoenchZmqServer.init_device","title":"<code>init_device()</code>","text":"<p>Initial tangoDS setup</p> Source code in <code>pytango-moenchZmqServer\\MoenchZmqServer.py</code> <pre><code>def init_device(self):\n\"\"\"Initial tangoDS setup\"\"\"\n    Device.init_device(self)\n    self.set_state(DevState.INIT)\n    self.get_device_properties(self.get_device_class())\n\n    self.reorder_table = np.load(\"reorder_table.npy\")\n    # sync manager for synchronization between threads\n    self._manager = mp.Manager()\n    # using simple mutex (lock) to synchronize\n    self._lock = self._manager.Lock()\n\n    # manager for allocation of shared memory between threads\n    self._shared_memory_manager = SharedMemoryManager()\n    # starting the shared memory manager\n    self._shared_memory_manager.start()\n    # default values of properties do not work without database though \u00af\\_(\u30c4)_/\u00af\n    processing_cores_amount = 16  # self.PROCESSING_CORES\n    zmq_ip = \"127.0.0.1\"  # self.ZMQ_RX_IP\n    zmq_port = \"50003\"  # self.ZMQ_RX_PORT\n\n    # using shared threadsafe Value instance from multiprocessing\n    self.shared_threshold = self._manager.Value(\"f\", 0)\n    self.shared_counting_threshold = self._manager.Value(\"f\", 0)\n    self.shared_server_running = self._manager.Value(\"b\", 0)\n    self.shared_processed_frames = self._manager.Value(\"I\", 0)\n    self.shared_received_frames = self._manager.Value(\"I\", 0)\n    self.shared_amount_frames = self._manager.Value(\"I\", 0)\n    self.shared_split_pump = self._manager.Value(\"b\", 0)\n\n    # calculating how many bytes need to be allocated and shared for a 400x400 float numpy array\n    img_bytes = np.zeros([400, 400], dtype=float).nbytes\n    # allocating 4 arrays of this type\n    self.shared_memory_pedestal = self._shared_memory_manager.SharedMemory(\n        size=img_bytes\n    )\n    self.shared_memory_analog_img = self._shared_memory_manager.SharedMemory(\n        size=img_bytes\n    )\n    self.shared_memory_threshold_img = self._shared_memory_manager.SharedMemory(\n        size=img_bytes\n    )\n    self.shared_memory_counting_img = self._shared_memory_manager.SharedMemory(\n        size=img_bytes\n    )\n    # creating thread pool executor to which the frame processing will be assigned\n    self._process_pool = ProcessPoolExecutor(processing_cores_amount)\n\n    # creating and initialing socket to read from\n    self._init_zmq_socket(zmq_ip, zmq_port)\n    loop = asyncio.get_event_loop()\n    loop.create_task(self.main())\n\n    # initialization of tango events for pictures buffers\n    self.set_change_event(\"analog_img\", True, False)\n    self.set_change_event(\"threshold_img\", True, False)\n    self.set_change_event(\"counting_img\", True, False)\n    self.set_state(DevState.ON)\n</code></pre>"},{"location":"reference/#MoenchZmqServer.MoenchZmqServer.save_files","title":"<code>save_files(path, filename, index)</code>","text":"<p>Function for saving the buffered images in .tiff format. The files will have different postfixes depending on processing mode.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>folder to save</p> required <code>filename</code> <code>str</code> <p>name to save</p> required <code>index</code> <code>str</code> <p>capture index</p> required Source code in <code>pytango-moenchZmqServer\\MoenchZmqServer.py</code> <pre><code>def save_files(self, path, filename, index):\n\"\"\"Function for saving the buffered images in .tiff format.\n    The files will have different postfixes depending on processing mode.\n\n    Args:\n        path (str): folder to save\n        filename (str): name to save\n        index (str): capture index\n    \"\"\"\n    savepath = os.path.join(path, filename)\n    if self.read_save_analog_img():\n        im = Image.fromarray(self.read_analog_img())\n        im.save(f\"{savepath}_{index}_analog.tiff\")\n\n    if self.read_save_threshold_img():\n        im = Image.fromarray(self.read_threshold_img())\n        im.save(f\"{savepath}_{index}_threshold_{self.read_threshold()}.tiff\")\n\n    if self.read_save_counting_img():\n        im = Image.fromarray(self.read_analog_img())\n        im.save(\n            f\"{savepath}_{index}_counting_{self.read_counting_threshold()}.tiff\"\n        )\n</code></pre>"}]}